{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9099d0ee",
   "metadata": {},
   "source": [
    "# Question Answering - Part 1\n",
    "\n",
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "392e53ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Computing\n",
    "import numpy as np\n",
    "\n",
    "# Data Management\n",
    "import pandas as pd\n",
    "\n",
    "# Datasets\n",
    "from datasets import get_dataset_config_names\n",
    "from datasets import load_dataset\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "\n",
    "# Transformers\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Java Script Object Notation\n",
    "import json\n",
    "\n",
    "# OS\n",
    "import os\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "\n",
    "# Evaluation Metrics\n",
    "from farm.evaluation.squad_evaluation import compute_f1, compute_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f913cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9216ebd2",
   "metadata": {},
   "source": [
    "### Retrieving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea960c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Uncomment and run this cell if you're on Colab or Kaggle\n",
    "!git clone https://github.com/nlp-with-transformers/notebooks.git\n",
    "%cd notebooks\n",
    "\n",
    "from install import *\n",
    "install_requirements(is_chapter7=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b2b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88789c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in [\"farm.utils\", \"farm.infer\", \"haystack.reader.farm.FARMReader\",\n",
    "              \"farm.modeling.prediction_head\", \"elasticsearch\", \"haystack.eval\",\n",
    "               \"haystack.document_store.base\", \"haystack.retriever.base\", \n",
    "              \"farm.data_handler.dataset\"]:\n",
    "    module_logger = logging.getLogger(module)\n",
    "    module_logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf510cb",
   "metadata": {},
   "source": [
    "## Question Answering\n",
    "\n",
    "### Building a Review-Based QA System\n",
    "\n",
    "#### The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56804fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = get_dataset_config_names(\"subjqa\")\n",
    "domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "497fa4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjqa = load_dataset(\"subjqa\", name=\"electronics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3920d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subjqa[\"train\"][\"answers\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b53370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {split: dset.to_pandas() for split, dset in subjqa.flatten().items()}\n",
    "\n",
    "for split, df in dfs.items():\n",
    "    print(f\"Number of questions in {split}: {df['id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96fd5419",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_cols = [\"title\", \"question\", \"answers.text\", \n",
    "           \"answers.answer_start\", \"context\"]\n",
    "sample_df = dfs[\"train\"][qa_cols].sample(2, random_state=7)\n",
    "\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f118b806",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = sample_df[\"answers.answer_start\"].iloc[0][0]\n",
    "end_idx = start_idx + len(sample_df[\"answers.text\"].iloc[0][0])\n",
    "\n",
    "sample_df[\"context\"].iloc[0][start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a558440",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "question_types = [\"What\", \"How\", \"Is\", \"Does\", \"Do\", \"Was\", \"Where\", \"Why\"]\n",
    "\n",
    "for q in question_types:\n",
    "    counts[q] = dfs[\"train\"][\"question\"].str.startswith(q).value_counts()[True]\n",
    "\n",
    "pd.Series(counts).sort_values().plot.barh()\n",
    "\n",
    "plt.title(\"Frequency of Question Types\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ec7b35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question_type in [\"How\", \"What\", \"Is\"]:\n",
    "    for question in (\n",
    "        dfs[\"train\"][dfs[\"train\"].question.str.startswith(question_type)]\n",
    "        .sample(n=3, random_state=42)['question']):\n",
    "        print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42df0a5",
   "metadata": {},
   "source": [
    "#### Tokenizing Text for QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00ff1fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"deepset/minilm-uncased-squad2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd90d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How much music can this hold?\"\n",
    "context = \"\"\"An MP3 is about 1 MB/minute, so about 6000 hours depending on \\\n",
    "file size.\"\"\"\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78ca04b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.DataFrame.from_dict(tokenizer(question, context), orient=\"index\")\n",
    "\n",
    "input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7eea330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(inputs[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2abf85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7fd2914",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c246af90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input IDs shape: {inputs.input_ids.size()}\")\n",
    "print(f\"Start logits shape: {start_logits.size()}\")\n",
    "print(f\"End logits shape: {end_logits.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89def59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_scores = start_logits.detach().numpy().flatten()\n",
    "e_scores = end_logits.detach().numpy().flatten()\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, sharex=True)\n",
    "colors = [\"C0\" if s != np.max(s_scores) else \"C1\" for s in s_scores]\n",
    "ax1.bar(x=tokens, height=s_scores, color=colors)\n",
    "ax1.set_ylabel(\"Start Scores\")\n",
    "colors = [\"C0\" if s != np.max(e_scores) else \"C1\" for s in e_scores]\n",
    "ax2.bar(x=tokens, height=e_scores, color=colors)\n",
    "ax2.set_ylabel(\"End Scores\")\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "906dad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = torch.argmax(start_logits)  \n",
    "end_idx = torch.argmax(end_logits) + 1  \n",
    "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
    "answer = tokenizer.decode(answer_span)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e0860f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "pipe(question=question, context=context, topk=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3a5f4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(question=\"Why is there no data?\", context=context, \n",
    "     handle_impossible_answer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2024abe3",
   "metadata": {},
   "source": [
    "#### Dealing with Long Passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d2ba891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_input_length(row):\n",
    "    inputs = tokenizer(row[\"question\"], row[\"context\"])\n",
    "    return len(inputs[\"input_ids\"])\n",
    "\n",
    "dfs[\"train\"][\"n_tokens\"] = dfs[\"train\"].apply(compute_input_length, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "dfs[\"train\"][\"n_tokens\"].hist(bins=100, grid=False, ec=\"C0\", ax=ax)\n",
    "plt.xlabel(\"Number of tokens in question-context pair\")\n",
    "ax.axvline(x=512, ymin=0, ymax=1, linestyle=\"--\", color=\"C1\", \n",
    "           label=\"Maximum sequence length\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63e6e1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dfs[\"train\"].iloc[0][[\"question\", \"context\"]]\n",
    "\n",
    "tokenized_example = tokenizer(example[\"question\"], example[\"context\"], \n",
    "                              return_overflowing_tokens=True, max_length=100, \n",
    "                              stride=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "321db495",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, window in enumerate(tokenized_example[\"input_ids\"]):\n",
    "    print(f\"Window #{idx} has {len(window)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a30adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for window in tokenized_example[\"input_ids\"]:\n",
    "    print(f\"{tokenizer.decode(window)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aded9a6",
   "metadata": {},
   "source": [
    "#### Initializing a Document Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8be424ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haystack\n",
    "from haystack import Label\n",
    "from haystack.eval import EvalAnswers\n",
    "from haystack.pipeline import Pipeline\n",
    "from haystack.eval import EvalDocuments\n",
    "from haystack.reader.farm import FARMReader\n",
    "from haystack.pipeline import ExtractiveQAPipeline\n",
    "from haystack.generator.transformers import RAGenerator\n",
    "from haystack.retriever.dense import DensePassageRetriever\n",
    "from haystack.retriever.sparse import ElasticsearchRetriever\n",
    "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61c4efe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"\"\"https://artifacts.elastic.co/downloads/elasticsearch/\\\n",
    "elasticsearch-7.9.2-linux-x86_64.tar.gz\"\"\"\n",
    "!wget -nc -q {url}\n",
    "!tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f36b1018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Elasticsearch as a background process\n",
    "!chown -R daemon:daemon elasticsearch-7.9.2\n",
    "es_server = Popen(args=['elasticsearch-7.9.2/bin/elasticsearch'],\n",
    "                  stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1))\n",
    "# Wait until Elasticsearch has started\n",
    "!sleep 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "606ec7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "launch_es()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87f926f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X GET \"localhost:9200/?pretty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "774f7e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = ElasticsearchDocumentStore(return_embedding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d982f459",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(document_store.get_all_documents()) or len(document_store.get_all_labels()) > 0:\n",
    "    document_store.delete_documents(\"document\")\n",
    "    document_store.delete_documents(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5112c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, df in dfs.items():\n",
    "    # Exclude duplicate reviews\n",
    "    docs = [{\"text\": row[\"context\"], \n",
    "             \"meta\":{\"item_id\": row[\"title\"], \"question_id\": row[\"id\"], \n",
    "                     \"split\": split}} \n",
    "        for _,row in df.drop_duplicates(subset=\"context\").iterrows()]\n",
    "    document_store.write_documents(docs, index=\"document\")\n",
    "    \n",
    "print(f\"Loaded {document_store.get_document_count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec86a2c",
   "metadata": {},
   "source": [
    "#### Initializing a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f948a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_retriever = ElasticsearchRetriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84bd9294",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id = \"B0074BW614\"\n",
    "query = \"Is it good for reading?\"\n",
    "retrieved_docs = es_retriever.retrieve(\n",
    "    query=query, top_k=3, filters={\"item_id\":[item_id], \"split\":[\"train\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6677758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retrieved_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed0fc63",
   "metadata": {},
   "source": [
    "#### Initializing a Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ad1bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"deepset/minilm-uncased-squad2\"\n",
    "max_seq_length, doc_stride = 384, 128\n",
    "reader = FARMReader(model_name_or_path=model_ckpt, progress_bar=False,\n",
    "                    max_seq_len=max_seq_length, doc_stride=doc_stride, \n",
    "                    return_no_answer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47f1bb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reader.predict_on_texts(question=question, texts=[context], top_k=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7fb4e",
   "metadata": {},
   "source": [
    "#### Putting it All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2dbe846",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = ExtractiveQAPipeline(reader, es_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "20909770",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_answers = 3\n",
    "preds = pipe.run(query=query, top_k_retriever=3, top_k_reader=n_answers,\n",
    "                 filters={\"item_id\": [item_id], \"split\":[\"train\"]})\n",
    "\n",
    "print(f\"Question: {preds['query']} \\n\")\n",
    "for idx in range(n_answers):\n",
    "    print(f\"Answer {idx+1}: {preds['answers'][idx]['answer']}\")\n",
    "    print(f\"Review snippet: ...{preds['answers'][idx]['context']}...\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f682ee71",
   "metadata": {},
   "source": [
    "### Improving Our QA Pipeline\n",
    "\n",
    "#### Evaluating The Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "64938b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalRetrieverPipeline:\n",
    "    def __init__(self, retriever):\n",
    "        self.retriever = retriever\n",
    "        self.eval_retriever = EvalDocuments()\n",
    "        pipe = Pipeline()\n",
    "        pipe.add_node(component=self.retriever, name=\"ESRetriever\", \n",
    "                      inputs=[\"Query\"])\n",
    "        pipe.add_node(component=self.eval_retriever, name=\"EvalRetriever\", \n",
    "                      inputs=[\"ESRetriever\"])\n",
    "        self.pipeline = pipe\n",
    "\n",
    "\n",
    "pipe = EvalRetrieverPipeline(es_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5fd29b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "for i, row in dfs[\"test\"].iterrows():\n",
    "    # Metadata used for filtering in the Retriever\n",
    "    meta = {\"item_id\": row[\"title\"], \"question_id\": row[\"id\"]}\n",
    "    # Populate labels for questions with answers\n",
    "    if len(row[\"answers.text\"]):\n",
    "        for answer in row[\"answers.text\"]:\n",
    "            label = Label(\n",
    "                question=row[\"question\"], answer=answer, id=i, origin=row[\"id\"],\n",
    "                meta=meta, is_correct_answer=True, is_correct_document=True,\n",
    "                no_answer=False)\n",
    "            labels.append(label)\n",
    "    # Populate labels for questions without answers\n",
    "    else:\n",
    "        label = Label(\n",
    "            question=row[\"question\"], answer=\"\", id=i, origin=row[\"id\"],\n",
    "            meta=meta, is_correct_answer=True, is_correct_document=True,\n",
    "            no_answer=True)  \n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "36c64ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e580a444",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.write_labels(labels, index=\"label\")\n",
    "\n",
    "print(f\"\"\"Loaded {document_store.get_label_count(index=\"label\")} \\\n",
    "question-answer pairs\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84546ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_agg = document_store.get_all_labels_aggregated(\n",
    "    index=\"label\",\n",
    "    open_domain=True,\n",
    "    aggregate_by_meta=[\"item_id\"]\n",
    ")\n",
    "\n",
    "print(len(labels_agg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "568522b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_agg[109])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e34420c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(pipeline, top_k_retriever=10, top_k_reader=4):\n",
    "    for l in labels_agg:\n",
    "        _ = pipeline.pipeline.run(\n",
    "            query=l.question,\n",
    "            top_k_retriever=top_k_retriever,\n",
    "            top_k_reader=top_k_reader,\n",
    "            top_k_eval_documents=top_k_retriever,    \n",
    "            labels=l,\n",
    "            filters={\"item_id\": [l.meta[\"item_id\"]], \"split\": [\"test\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b67be14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_pipeline(pipe, top_k_retriever=3)\n",
    "print(f\"Recall@3: {pipe.eval_retriever.recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "de9efb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retriever(retriever, topk_values = [1,3,5,10,20]):\n",
    "    topk_results = {}\n",
    "\n",
    "    for topk in topk_values:\n",
    "        # Create Pipeline\n",
    "        p = EvalRetrieverPipeline(retriever)\n",
    "        # Loop over each question-answers pair in test set\n",
    "        run_pipeline(p, top_k_retriever=topk)\n",
    "        # Get metrics\n",
    "        topk_results[topk] = {\"recall\": p.eval_retriever.recall}\n",
    "        \n",
    "    return pd.DataFrame.from_dict(topk_results, orient=\"index\")\n",
    "\n",
    "\n",
    "es_topk_df = evaluate_retriever(es_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "11532520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_retriever_eval(dfs, retriever_names):\n",
    "    fig, ax = plt.subplots()\n",
    "    for df, retriever_name in zip(dfs, retriever_names):\n",
    "        df.plot(y=\"recall\", ax=ax, label=retriever_name)\n",
    "    plt.xticks(df.index)\n",
    "    plt.ylabel(\"Top-k Recall\")\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_retriever_eval([es_topk_df], [\"BM25\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e6df94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpr_retriever = DensePassageRetriever(document_store=document_store,\n",
    "    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "    embed_title=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2d37cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.update_embeddings(retriever=dpr_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c7d81a",
   "metadata": {},
   "source": [
    "#### Evaluating The Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "31ce9bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = \"about 6000 hours\"\n",
    "label = \"6000 hours\"\n",
    "\n",
    "print(f\"EM: {compute_exact(label, pred)}\")\n",
    "print(f\"F1: {compute_f1(label, pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fa923bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = \"about 6000 dollars\"\n",
    "\n",
    "print(f\"EM: {compute_exact(label, pred)}\")\n",
    "print(f\"F1: {compute_f1(label, pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9a211b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_reader(reader):\n",
    "    score_keys = ['top_1_em', 'top_1_f1']\n",
    "    eval_reader = EvalAnswers(skip_incorrect_retrieval=False)\n",
    "    pipe = Pipeline()\n",
    "    pipe.add_node(component=reader, name=\"QAReader\", inputs=[\"Query\"])\n",
    "    pipe.add_node(component=eval_reader, name=\"EvalReader\", inputs=[\"QAReader\"])\n",
    "\n",
    "    for l in labels_agg:\n",
    "        doc = document_store.query(l.question, \n",
    "                                   filters={\"question_id\":[l.origin]})\n",
    "        _ = pipe.run(query=l.question, documents=doc, labels=l)\n",
    "                \n",
    "    return {k:v for k,v in eval_reader.__dict__.items() if k in score_keys}\n",
    "\n",
    "reader_eval = {}\n",
    "reader_eval[\"Fine-tune on SQuAD\"] = evaluate_reader(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8b101d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reader_eval(reader_eval):\n",
    "    fig, ax = plt.subplots()\n",
    "    df = pd.DataFrame.from_dict(reader_eval)\n",
    "    df.plot(kind=\"bar\", ylabel=\"Score\", rot=0, ax=ax)\n",
    "    ax.set_xticklabels([\"EM\", \"F1\"])\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "plot_reader_eval(reader_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63254d1",
   "metadata": {},
   "source": [
    "#### Domain Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f3d8b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paragraphs(df):\n",
    "    paragraphs = []\n",
    "    id2context = dict(zip(df[\"review_id\"], df[\"context\"]))\n",
    "    for review_id, review in id2context.items():\n",
    "        qas = []\n",
    "        # Filter for all question-answer pairs about a specific context\n",
    "        review_df = df.query(f\"review_id == '{review_id}'\")\n",
    "        id2question = dict(zip(review_df[\"id\"], review_df[\"question\"]))\n",
    "        # Build up the qas array\n",
    "        for qid, question in id2question.items():\n",
    "            # Filter for a single question ID\n",
    "            question_df = df.query(f\"id == '{qid}'\").to_dict(orient=\"list\")\n",
    "            ans_start_idxs = question_df[\"answers.answer_start\"][0].tolist()\n",
    "            ans_text = question_df[\"answers.text\"][0].tolist()\n",
    "            # Fill answerable questions\n",
    "            if len(ans_start_idxs):\n",
    "                answers = [\n",
    "                    {\"text\": text, \"answer_start\": answer_start}\n",
    "                    for text, answer_start in zip(ans_text, ans_start_idxs)]\n",
    "                is_impossible = False\n",
    "            else:\n",
    "                answers = []\n",
    "                is_impossible = True\n",
    "            # Add question-answer pairs to qas\n",
    "            qas.append({\"question\": question, \"id\": qid, \n",
    "                        \"is_impossible\": is_impossible, \"answers\": answers})\n",
    "        # Add context and question-answer pairs to paragraphs\n",
    "        paragraphs.append({\"qas\": qas, \"context\": review})\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c081e3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = dfs[\"train\"].query(\"title == 'B00001P4ZH'\")\n",
    "\n",
    "create_paragraphs(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a1efc2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_squad(dfs):\n",
    "    for split, df in dfs.items():\n",
    "        subjqa_data = {}\n",
    "        # Create `paragraphs` for each product ID\n",
    "        groups = (df.groupby(\"title\").apply(create_paragraphs)\n",
    "            .to_frame(name=\"paragraphs\").reset_index())\n",
    "        subjqa_data[\"data\"] = groups.to_dict(orient=\"records\")\n",
    "        # Save the result to disk\n",
    "        with open(f\"electronics-{split}.json\", \"w+\", encoding=\"utf-8\") as f:\n",
    "            json.dump(subjqa_data, f)\n",
    "            \n",
    "convert_to_squad(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7201f517",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = \"electronics-train.json\"\n",
    "dev_filename = \"electronics-validation.json\"\n",
    "\n",
    "reader.train(data_dir=\".\", use_gpu=True, n_epochs=1, batch_size=16,\n",
    "             train_filename=train_filename, dev_filename=dev_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "65c348b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_eval[\"Fine-tune on SQuAD + SubjQA\"] = evaluate_reader(reader)\n",
    "plot_reader_eval(reader_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4192e3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "minilm_ckpt = \"microsoft/MiniLM-L12-H384-uncased\"\n",
    "\n",
    "minilm_reader = FARMReader(model_name_or_path=minilm_ckpt, progress_bar=False,\n",
    "                           max_seq_len=max_seq_length, doc_stride=doc_stride,\n",
    "                           return_no_answer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "72915b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "minilm_reader.train(data_dir=\".\", use_gpu=True, n_epochs=1, batch_size=16,\n",
    "             train_filename=train_filename, dev_filename=dev_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8146b278",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_eval[\"Fine-tune on SubjQA\"] = evaluate_reader(minilm_reader)\n",
    "plot_reader_eval(reader_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b2f80",
   "metadata": {},
   "source": [
    "#### Evaluating the Whole QA Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a1e16403",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = EvalRetrieverPipeline(es_retriever)\n",
    "# Add nodes for reader\n",
    "eval_reader = EvalAnswers()\n",
    "pipe.pipeline.add_node(component=reader, name=\"QAReader\", \n",
    "              inputs=[\"EvalRetriever\"])\n",
    "pipe.pipeline.add_node(component=eval_reader, name=\"EvalReader\", \n",
    "              inputs=[\"QAReader\"])\n",
    "# Evaluate!\n",
    "run_pipeline(pipe)\n",
    "# Extract metrics from reader\n",
    "reader_eval[\"QA Pipeline (top-1)\"] = {\n",
    "    k:v for k,v in eval_reader.__dict__.items()\n",
    "    if k in [\"top_1_em\", \"top_1_f1\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d18a412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#caption Comparison of EM and _F_~1~ scores for the reader against the whole QA pipeline\n",
    "plot_reader_eval({\"Reader\": reader_eval[\"Fine-tune on SQuAD + SubjQA\"], \n",
    "                  \"QA pipeline (top-1)\": reader_eval[\"QA Pipeline (top-1)\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19251f7a",
   "metadata": {},
   "source": [
    "### Going Beyond Extractive QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "851a310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = RAGenerator(model_name_or_path=\"facebook/rag-token-nq\",\n",
    "                        embed_title=False, num_beams=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1bd35df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = GenerativeQAPipeline(generator=generator, retriever=dpr_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c4937c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers(query, top_k_generator=3):\n",
    "    preds = pipe.run(query=query, top_k_generator=top_k_generator, \n",
    "                     top_k_retriever=5, filters={\"item_id\":[\"B0074BW614\"]})  \n",
    "    print(f\"Question: {preds['query']} \\n\")\n",
    "    for idx in range(top_k_generator):\n",
    "        print(f\"Answer {idx+1}: {preds['answers'][idx]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "13809d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answers(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "93d04175",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answers(\"What is the main drawback?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81941ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
