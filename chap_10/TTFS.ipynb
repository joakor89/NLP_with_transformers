{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f1ffef6",
   "metadata": {},
   "source": [
    "# Training Transformers from Scratch\n",
    "\n",
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5abcbf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Numerical Computing \n",
    "# import numpy as np\n",
    "\n",
    "# # Data Manipulation\n",
    "# import pandas as pd\n",
    "\n",
    "# # Data Visualization \n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Transformers\n",
    "# from transformers import AutoTokenizer\n",
    "# from transformers import pipeline, set_seed\n",
    "# from transformers import AutoConfig, AutoModelForCausalLM\n",
    "# from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "\n",
    "# # PyTorch\n",
    "# import torch\n",
    "# from torch.utils.data import IterableDataset\n",
    "# from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "# # Datasets\n",
    "# from datasets import load_dataset, DownloadConfig\n",
    "\n",
    "# # Operating System\n",
    "# import keyword\n",
    "# import psutil, os\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# # Terminal Interface\n",
    "# from argparse import Namespace\n",
    "\n",
    "# # FlexSystem, Weight & Biases\n",
    "# import logging\n",
    "# import wandb\n",
    "\n",
    "# # Regular Expressions\n",
    "# import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd26a686",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "022696bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment and run this cell if you're on Colab or Kaggle\n",
    "# !git clone https://github.com/nlp-with-transformers/notebooks.git\n",
    "# %cd notebooks\n",
    "# from install import *\n",
    "# install_requirements(is_chapter10=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "123c24e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import *\n",
    "# setup_chapter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cda360a",
   "metadata": {},
   "source": [
    "## Large Datasets and Where to Find Them\n",
    "\n",
    "#### Challenges of Building a Large-Scale Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dffcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_gpt = pipeline(\"text-generation\", model=\"openai-gpt\")\n",
    "\n",
    "generation_gpt2 = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344cf635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_size(model):\n",
    "    return sum(t.numel() for t in model.parameters())\n",
    "\n",
    "print(f\"GPT  size: {model_size(generation_gpt.model)/1000**2:.1f}M parameters\")\n",
    "print(f\"GPT2 size: {model_size(generation_gpt2.model)/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7419c517",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea112bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enum_pipeline_ouputs(pipe, prompt, num_return_sequences):\n",
    "    out = pipe(prompt, num_return_sequences=num_return_sequences,\n",
    "               clean_up_tokenization_spaces=True)\n",
    "    return \"\\n\".join(f\"{i+1}.\" + s[\"generated_text\"] for i, s in enumerate(out))\n",
    "\n",
    "prompt = \"\\nWhen they came back\"\n",
    "print(\"GPT completions:\\n\" + enum_pipeline_ouputs(generation_gpt, prompt, 3))\n",
    "print(\"\")\n",
    "print(\"GPT-2 completions:\\n\" + enum_pipeline_ouputs(generation_gpt2, prompt, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3054a6f4",
   "metadata": {},
   "source": [
    "### Working with Large Datasets\n",
    "\n",
    "#### Memory mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0465ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_config = DownloadConfig(delete_extracted=True)\n",
    "\n",
    "dataset = load_dataset(\"./codeparrot\", split=\"train\",\n",
    "                       download_config=download_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c0cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of python files code in dataset : {len(dataset)}\")\n",
    "ds_size = sum(os.stat(f[\"filename\"]).st_size for f in dataset.cache_files)\n",
    "# os.stat.st_size is expressed in bytes, so we convert to GB\n",
    "print(f\"Dataset size (cache file) : {ds_size / 2**30:.2f} GB\")\n",
    "# Process.memory_info is expressed in bytes, so we convert to MB\n",
    "print(f\"RAM used: {psutil.Process(os.getpid()).memory_info().rss >> 20} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0bfe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamed_dataset = load_dataset('./codeparrot', split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4216e1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(streamed_dataset)\n",
    "\n",
    "print(dataset[0] == next(iterator))\n",
    "print(dataset[1] == next(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca599a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_dataset = load_dataset('transformersbook/codeparrot', split=\"train\",\n",
    "                              streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec6bfd6",
   "metadata": {},
   "source": [
    "### Building a Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83fa9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_list(tokenizer, string):\n",
    "    input_ids = tokenizer(string, add_special_tokens=False)[\"input_ids\"]\n",
    "    return [tokenizer.decode(tok) for tok in input_ids]\n",
    "\n",
    "tokenizer_T5 = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "tokenizer_camembert = AutoTokenizer.from_pretrained(\"camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d849d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'T5 tokens for \"sex\": {tok_list(tokenizer_T5,\"sex\")}')\n",
    "print(f'CamemBERT tokens for \"being\": {tok_list(tokenizer_camembert,\"being\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13e0fea",
   "metadata": {},
   "source": [
    "### Measuring Tokenizer Performance\n",
    "\n",
    "#### A Tokenizer for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceb78e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_code = r\"\"\"def say_hello():\n",
    "    print(\"Hello, World!\")\n",
    "# Print it\n",
    "say_hello()\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4433ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.backend_tokenizer.normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5234a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e490cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, e = u\"a\", u\"â‚¬\"\n",
    "\n",
    "byte = ord(a.encode(\"utf-8\"))\n",
    "print(f'`{a}` is encoded as `{a.encode(\"utf-8\")}` with a single byte: {byte}')\n",
    "\n",
    "byte = [ord(chr(i)) for i in e.encode(\"utf-8\")]\n",
    "print(f'`{e}` is encoded as `{e.encode(\"utf-8\")}` with three bytes: {byte}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27557f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_to_unicode_map = bytes_to_unicode()\n",
    "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
    "base_vocab = list(unicode_to_byte_map.keys())\n",
    "\n",
    "print(f'Size of our base vocabulary: {len(base_vocab)}')\n",
    "print(f'First element: `{base_vocab[0]}`, last element: `{base_vocab[-1]}`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7010bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_to_unicode_map = bytes_to_unicode()\n",
    "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
    "base_vocab = list(unicode_to_byte_map.keys())\n",
    "\n",
    "examples = [\n",
    "    ['Regular characters', '`a` and `?`', f'{ord(\"a\")} and {ord(\"?\")}' , f'`{byte_to_unicode_map[ord(\"a\")]}` and `{byte_to_unicode_map[ord(\"?\")]}`'],\n",
    "    ['Nonprintable control character (carriage return)', '`U+000D`', f'13', f'`{byte_to_unicode_map[13]}`'],\n",
    "    ['A space', '` `', f'{ord(\" \")}', f'`{byte_to_unicode_map[ord(\" \")]}`'],\n",
    "    ['A nonbreakable space', '`\\xa0`', '160', f'`{byte_to_unicode_map[ord(chr(160))]}`'],\n",
    "    ['A newline character', '`\\n`', '10', f'`{byte_to_unicode_map[ord(chr(10))]}`'],\n",
    "]\n",
    "\n",
    "pd.DataFrame(examples, columns = ['Description', 'Character', 'Bytes', 'Mapped bytes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c010c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6ade06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Size of the vocabulary: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b6098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a974b8b3",
   "metadata": {},
   "source": [
    "### Training a Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e16965",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sorted(tokenizer.vocab.items(), key=lambda x: len(x[0]), reverse=True)\n",
    "print([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[:8]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6049a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sorted(tokenizer.vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "print([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[:12]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f01620",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sorted(new_tokenizer.vocab.items(), key=lambda x: x[1], reverse=False)\n",
    "print([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[257:280]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75fd0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([f'{new_tokenizer.convert_tokens_to_string(t)}' for t,_ in tokens[-12:]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1992531",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186da393",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are in total {len(keyword.kwlist)} Python keywords.')\n",
    "for keyw in keyword.kwlist:\n",
    "    if keyw not in new_tokenizer.vocab:\n",
    "        print(f'No, keyword `{keyw}` is not in the vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab454d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 200000\n",
    "new_tokenizer_larger = tokenizer.train_new_from_iterator(batch_iterator(),\n",
    "    vocab_size=32768, initial_alphabet=base_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aab75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sorted(new_tokenizer_larger.vocab.items(), key=lambda x: x[1],\n",
    "                reverse=False)\n",
    "print([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[-12:]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ef56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_tokenizer_larger(python_code).tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b1743",
   "metadata": {},
   "outputs": [],
   "source": [
    "for keyw in keyword.kwlist:\n",
    "    if keyw not in new_tokenizer_larger.vocab:\n",
    "        print(f'No, keyword `{keyw}` is not in the vocabulary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d515839",
   "metadata": {},
   "source": [
    "### Saving a Custom Tokenizer on the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f4759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"codeparrot\"\n",
    "org = \"transformersbook\"\n",
    "new_tokenizer_larger.push_to_hub(model_ckpt, organization=org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13119218",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_tokenizer = AutoTokenizer.from_pretrained(org + \"/\" + model_ckpt)\n",
    "print(reloaded_tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785a7d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer.push_to_hub(model_ckpt+ \"-small-vocabulary\", organization=org)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900b868a",
   "metadata": {},
   "source": [
    "### Training a Model from Scratch\n",
    "\n",
    "#### Initializing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1fbeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(org + \"/\" + model_ckpt)\n",
    "config = AutoConfig.from_pretrained(\"gpt2-xl\", vocab_size=len(tokenizer))\n",
    "model = AutoModelForCausalLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0dfb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'GPT-2 (xl) size: {model_size(model)/1000**2:.1f}M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc69e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"models/\" + model_ckpt, push_to_hub=True,\n",
    "                      organization=org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8ac6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "config_small = AutoConfig.from_pretrained(\"gpt2\", vocab_size=len(tokenizer))\n",
    "model_small = AutoModelForCausalLM.from_config(config_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550cf595",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'GPT-2 size: {model_size(model_small)/1000**2:.1f}M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3570f525",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_small.save_pretrained(\"models/\" + model_ckpt + \"-small\", push_to_hub=True,\n",
    "                            organization=org)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138df90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, total_characters, total_tokens = 500, 0, 0\n",
    "dataset = load_dataset('transformersbook/codeparrot-train', split='train',\n",
    "                       streaming=True)\n",
    "\n",
    "for _, example in tqdm(zip(range(examples), iter(dataset)), total=examples):\n",
    "    total_characters += len(example['content'])\n",
    "    total_tokens += len(tokenizer(example['content']).tokens())\n",
    "\n",
    "characters_per_token = total_characters / total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5d5224",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(characters_per_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacfaa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantLengthDataset(IterableDataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, dataset, seq_length=1024,\n",
    "                 num_of_sequences=1024, chars_per_token=3.6):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.concat_token_id = tokenizer.eos_token_id\n",
    "        self.dataset = dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.input_characters = seq_length * chars_per_token * num_of_sequences\n",
    "    \n",
    "    def __iter__(self):\n",
    "        iterator = iter(self.dataset)\n",
    "        more_examples = True\n",
    "        while more_examples:\n",
    "            buffer, buffer_len = [], 0\n",
    "            while True:\n",
    "                if buffer_len >= self.input_characters:\n",
    "                    m=f\"Buffer full: {buffer_len}>={self.input_characters:.0f}\"\n",
    "                    print(m)\n",
    "                    break\n",
    "                try:\n",
    "                    m=f\"Fill buffer: {buffer_len}<{self.input_characters:.0f}\"\n",
    "                    print(m)\n",
    "                    buffer.append(next(iterator)[\"content\"])\n",
    "                    buffer_len += len(buffer[-1])\n",
    "                except StopIteration:\n",
    "                    iterator = iter(self.dataset)\n",
    "\n",
    "            all_token_ids = []\n",
    "            tokenized_inputs = self.tokenizer(buffer, truncation=False)\n",
    "            for tokenized_input in tokenized_inputs['input_ids']:\n",
    "                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n",
    "            \n",
    "            for i in range(0, len(all_token_ids), self.seq_length):\n",
    "                input_ids = all_token_ids[i : i + self.seq_length]\n",
    "                if len(input_ids) == self.seq_length:\n",
    "                    yield torch.tensor(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447e7327",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = dataset.shuffle(buffer_size=100)\n",
    "constant_length_dataset = ConstantLengthDataset(tokenizer, shuffled_dataset,\n",
    "                                                num_of_sequences=10)\n",
    "dataset_iterator = iter(constant_length_dataset)\n",
    "\n",
    "lengths = [len(b) for _, b in zip(range(5), dataset_iterator)]\n",
    "print(f\"Lengths of the sequences: {lengths}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cae1d60",
   "metadata": {},
   "source": [
    "### Defining the Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722afddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"train_batch_size\": 2, # 12\n",
    "          \"valid_batch_size\": 2, # 12\n",
    "          \"weight_decay\": 0.1,\n",
    "          \"shuffle_buffer\": 1000,\n",
    "          \"learning_rate\": 2e-4, # 5e-4\n",
    "          \"lr_scheduler_type\": \"cosine\",\n",
    "          \"num_warmup_steps\": 750, # 2000\n",
    "          \"gradient_accumulation_steps\": 16, # 1\n",
    "          \"max_train_steps\": 50000, # 150000\n",
    "          \"max_eval_steps\": -1,\n",
    "          \"seq_length\": 1024,\n",
    "          \"seed\": 1,\n",
    "          \"save_checkpoint_steps\": 50000} # 15000\n",
    "\n",
    "args = Namespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98d7cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(project_name):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO, handlers=[\n",
    "        logging.FileHandler(f\"log/debug_{accelerator.process_index}.log\"),\n",
    "        logging.StreamHandler()])\n",
    "    if accelerator.is_main_process: # We only want to set up logging once\n",
    "        wandb.init(project=project_name, config=args)\n",
    "        run_name = wandb.run.name\n",
    "        tb_writer = SummaryWriter()\n",
    "        tb_writer.add_hparams(vars(args), {'0': 0})\n",
    "        logger.setLevel(logging.INFO)\n",
    "        datasets.utils.logging.set_verbosity_debug()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        tb_writer = None\n",
    "        run_name = ''\n",
    "        logger.setLevel(logging.ERROR)\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "    return logger, tb_writer, run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba2af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(step, metrics):\n",
    "    logger.info(f\"Step {step}: {metrics}\")\n",
    "    if accelerator.is_main_process:\n",
    "        wandb.log(metrics)\n",
    "        [tb_writer.add_scalar(k, v, step) for k, v in metrics.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e217ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [{'params': params_with_wd, 'weight_decay': args.weight_decay},\n",
    "            {'params': params_without_wd, 'weight_decay': 0.0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1b1439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch, labels=batch)\n",
    "        loss = outputs.loss.repeat(args.valid_batch_size)\n",
    "        losses.append(accelerator.gather(loss))\n",
    "        if args.max_eval_steps > 0 and step >= args.max_eval_steps: break\n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "\t\tperplexity = torch.exp(loss)\n",
    "\texcept OverflowError:\n",
    "\t\tperplexity = torch.tensor(float(\"inf\"))\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea8887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(args.seed)\n",
    "\n",
    "# Accelerator\n",
    "accelerator = Accelerator()\n",
    "samples_per_step = accelerator.state.num_processes * args.train_batch_size\n",
    "\n",
    "# Logging\n",
    "logger, tb_writer, run_name = setup_logging(project_name.split(\"/\")[1])\n",
    "logger.info(accelerator.state)\n",
    "\n",
    "# Load model and tokenizer\n",
    "if accelerator.is_main_process:\n",
    "    hf_repo = Repository(\"./\", clone_from=project_name, revision=run_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./\", gradient_checkpointing=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./\")\n",
    "\n",
    "# Load dataset and dataloader\n",
    "train_dataloader, eval_dataloader = create_dataloaders(dataset_name)\n",
    "\n",
    "# Prepare the optimizer and learning rate scheduler\n",
    "optimizer = AdamW(get_grouped_params(model), lr=args.learning_rate)\n",
    "lr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer,\n",
    "                             num_warmup_steps=args.num_warmup_steps,\n",
    "                             num_training_steps=args.max_train_steps,)\n",
    "def get_lr():\n",
    "    return optimizer.param_groups[0]['lr']\n",
    "\n",
    "# Prepare everything with our `accelerator` (order of args is not important)\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader)\n",
    "\n",
    "# Train model\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "for step, batch in enumerate(train_dataloader, start=1):\n",
    "    loss = model(batch, labels=batch).loss\n",
    "    log_metrics(step, {'lr': get_lr(), 'samples': step*samples_per_step,\n",
    "                       'steps': completed_steps, 'loss/train': loss.item()})\n",
    "    loss = loss / args.gradient_accumulation_steps\n",
    "    accelerator.backward(loss)\n",
    "    if step % args.gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        completed_steps += 1\n",
    "    if step % args.save_checkpoint_steps == 0:\n",
    "        logger.info('Evaluating and saving model checkpoint')\n",
    "        eval_loss, perplexity = evaluate()\n",
    "        log_metrics(step, {'loss/eval': eval_loss, 'perplexity': perplexity})\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        if accelerator.is_main_process:\n",
    "            unwrapped_model.save_pretrained(\"./\")\n",
    "            hf_repo.push_to_hub(commit_message=f'step {step}')\n",
    "        model.train()\n",
    "    if completed_steps >= args.max_train_steps:\n",
    "        break\n",
    "\n",
    "# Evaluate and save the last checkpoint\n",
    "logger.info('Evaluating and saving model after training')\n",
    "eval_loss, perplexity = evaluate()\n",
    "log_metrics(step, {'loss/eval': eval_loss, 'perplexity': perplexity})\n",
    "accelerator.wait_for_everyone()\n",
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "if accelerator.is_main_process:\n",
    "    unwrapped_model.save_pretrained(\"./\")\n",
    "    hf_repo.push_to_hub(commit_message=f'final model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06036ef2",
   "metadata": {},
   "source": [
    "### Results & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a597f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = 'transformersbook/codeparrot-small'\n",
    "generation = pipeline('text-generation', model=model_ckpt, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e58f164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_block(string):\n",
    "    return re.split('\\nclass|\\ndef|\\n#|\\n@|\\nprint|\\nif', string)[0].rstrip()\n",
    "\n",
    "def complete_code(pipe, prompt, max_length=64, num_completions=4, seed=1):\n",
    "    set_seed(seed)\n",
    "    gen_kwargs = {\"temperature\":0.4, \"top_p\":0.95, \"top_k\":0, \"num_beams\":1,\n",
    "                  \"do_sample\":True,}\n",
    "    code_gens = generation(prompt, num_return_sequences=num_completions, \n",
    "                            max_length=max_length, **gen_kwargs)\n",
    "    code_strings = []\n",
    "    for code_gen in code_gens:\n",
    "        generated_code = first_block(code_gen['generated_text'][len(prompt):])\n",
    "        code_strings.append(generated_code)\n",
    "    print(('\\n'+'='*80 + '\\n').join(code_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e75a481",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''def area_of_rectangle(a: float, b: float):\n",
    "    \"\"\"Return the area of the rectangle.\"\"\"'''\n",
    "complete_code(generation, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dcdc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''def get_urls_from_html(html):\n",
    "    \"\"\"Get all embedded URLs in a HTML string.\"\"\"'''\n",
    "complete_code(generation, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c52c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = 'transformersbook/codeparrot'\n",
    "generation = pipeline('text-generation', model=model_ckpt, device=0)\n",
    "\n",
    "prompt = '''# a function in native python:\n",
    "def mean(a):\n",
    "    return sum(a)/len(a)\n",
    "\n",
    "# the same function using numpy:\n",
    "import numpy as np\n",
    "def mean(a):'''\n",
    "complete_code(generation, prompt, max_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a6a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''X = np.random.randn(100, 100)\n",
    "y = np.random.randint(0, 1, 100)\n",
    "\n",
    "# fit random forest classifier with 20 estimators'''\n",
    "complete_code(generation, prompt, max_length=96)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
